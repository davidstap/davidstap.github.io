<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | David Stap</title>
    <link>https://davidstap.github.io/post/</link>
      <atom:link href="https://davidstap.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>¬© 2020 David Stap</copyright><lastBuildDate>Mon, 23 Nov 2020 00:00:00 +0100</lastBuildDate>
    <image>
      <url>https://davidstap.github.io/images/icon_hud4b034c4f7a5c231da985da63cf83ade_53523_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>https://davidstap.github.io/post/</link>
    </image>
    
    <item>
      <title>Some EMNLP 2020 notes üèùÔ∏èüá©üá¥üíª</title>
      <link>https://davidstap.github.io/post/emnlp/</link>
      <pubDate>Mon, 23 Nov 2020 00:00:00 +0100</pubDate>
      <guid>https://davidstap.github.io/post/emnlp/</guid>
      <description>&lt;p&gt;
&lt;a href=&#34;https://2020.emnlp.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EMNLP 2020&lt;/a&gt;, my second online NLP conference, is over. An improvement over 
&lt;a href=&#34;https://acl2020.org&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ACL&lt;/a&gt; were the joint Q&amp;amp;A Zoom sessions (no more awkward empty rooms). Even better was 
&lt;a href=&#34;https://gather.town&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gather.Town&lt;/a&gt; (which reminded me of 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Habbo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Habbo Hotel&lt;/a&gt;). It was fun to walk around from poster to poster. Let&amp;rsquo;s see what future conferences bring! Below are some notes for 20 papers that I found interesting:&lt;/p&gt;
&lt;h2 id=&#34;interesting-papers-presented-at-emnlp-2020&#34;&gt;Interesting papers presented at EMNLP 2020&lt;/h2&gt;
&lt;h3 id=&#34;-bridging-linguistic-typology-and-multilingual-machine-translation-with-multi-view-language-representationshttpswwwaclweborganthology2020emnlp-main187-span-stylefont-size16pxoncevay-et-alspan&#34;&gt;‚≠ê 
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.187&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bridging linguistic typology and multilingual machine translation with multi-view language representations&lt;/a&gt; &lt;span style=&#34;font-size:16px;&#34;&gt;(Oncevay et al.)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;The authors show that Singular Vector Canonical Correlation Analysis (SVCCA) &lt;span style=&#34;font-size:16px;&#34;&gt;
&lt;a href=&#34;https://proceedings.neurips.cc/paper/2017/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Raghu et al., 2017)&lt;/a&gt;&lt;/span&gt; can be used to fuse linguistic typology knowledge base features with multilingual NMT-leared embeddings without diminishing the encoded original information. They show that this method can be successfully applied to language clustering, where the main idea is to obtain smaller multilingual models (for many-to-one) as an intermediate point between maintaining many pairwise systems and a single massively multilingual model. In earlier work, SVCCA is also used to study massively multilingual NMT representations &lt;span style=&#34;font-size:16px;&#34;&gt;
&lt;a href=&#34;https://www.aclweb.org/anthology/D19-1167&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Kudugunta et al., 2019)&lt;/a&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;-complete-multilingual-neural-machine-translationhttpswwwaclweborganthology2020wmt-166-span-stylefont-size16pxfreitag-and-firatspan&#34;&gt;‚≠ê 
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.wmt-1.66&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Complete multilingual neural machine translation&lt;/a&gt; &lt;span style=&#34;font-size:16px;&#34;&gt;(Freitag and Firat)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Freitag and Firat move away from English-centric multilingual data to alleviate zero-shot translation errors. They do this by mining hidden multi-way aligned examples from an English-centric dataset, and succeed in generating a complete training graph between all languages. Results indicate that translations between non-English languages benefit from the multi-way data. It is great to know that, besides data that is multi-way by design, one can obtain a lot of multi-way data hidden in parallel corpora. The same trick could be applied to other high-resource languages that act as a bridge, such as Spanish.&lt;/p&gt;
&lt;h3 id=&#34;-revisiting-modularized-multilingual-nmt-to-meet-industrial-demandshttpswwwaclweborganthology2020emnlp-main476-span-stylefont-size16pxlyu-et-alspan&#34;&gt;‚≠ê 
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.476&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Revisiting modularized multilingual NMT to meet industrial demands&lt;/a&gt; &lt;span style=&#34;font-size:16px;&#34;&gt;(Lyu et al.)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;The complete parameter sharing approach is currently dominant in multilingual machine translation, but it suffers from a capacity bottleneck and adding new languages is troublesome because retraining is required. In this paper the authors revisit the modularized setup (language specific encoders and decoders) and argue for its appropriateness in industry. They propose a model that is trained on all parallel directions (as opposed to English-centric) without sharing attention among decoders (because the semantic space is already implicitly shared by training on all language directions). That is, they use multi-way aligned data exclusively. This approach outperforms complete sharing and bilingual approaches. Additionally, in the modularized setup high-resource languages benefit from low-resource languages, which is not the case for complete sharing. Finally, the authors find that the performance of zero-shot directions outperform pivoted baselines.&lt;/p&gt;
&lt;h3 id=&#34;-participatory-research-for-low-resourced-machine-translation-a-case-study-in-african-languageshttpswwwaclweborganthology2020findings-emnlp195-span-stylefont-size16pxnekoto-et-alspan&#34;&gt;‚≠ê 
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.findings-emnlp.195&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Participatory research for low-resourced machine translation: a case study in African languages&lt;/a&gt; &lt;span style=&#34;font-size:16px;&#34;&gt;(Nekoto et al.)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;The authors (47!) argue that low-resourcedness is a complex problem going beyond data availability and reflects systemic problems in society. Machine translation researchers cannot solve this problem alone, therefore participatory research is introduced as a means to involve all necessary stakeholders required in the machine translation development process. The feasibility and scalability of this approach is demonstrated with a case study on machine translation for African languages, the result of which is a collection of novel translation datasets and benchmarks for over 30 African languages. To learn more, check out their great WMT keynote.&lt;/p&gt;
&lt;h3 id=&#34;-on-negative-interference-in-multilingual-models-findings-and-a-meta-learning-treatmenthttpswwwaclweborganthology2020emnlp-main359-span-stylefont-size16pxwang-et-alspan&#34;&gt;‚≠ê 
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.359&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On negative interference in multilingual models: findings and a meta-learning treatment&lt;/a&gt; &lt;span style=&#34;font-size:16px;&#34;&gt;(Wang et al.)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;In this paper, two possible sources for negative interference in a multilingual language model (XLM-R &lt;span style=&#34;font-size:16px;&#34;&gt;
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.536&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Conneau et al., 2020b)&lt;/a&gt;&lt;/span&gt;) are identified: 1) by sampling batches during training and computing gradient cosine similarities during training, the authors find that gradient conflict does exist for dissimilar languages, and they roughly reflect language proximity; 2) the authors observe language-specific parameters which are a potential cause for interference, but adding language-specific parameters hurts zero-shot transfer. Guided by these insights, the authors propose a meta-learning objective that effectively mitigates negative interference and improves transferability of shared layers. This objective requires calculating higher order gradients, which enable signals from all languages passing through language specific layers without sacrificing transferability in non-shared layers. Note that this work studies a bilingual setup, and it is unclear whether the approach scales to a larger number of languages.&lt;/p&gt;
&lt;h3 id=&#34;-do-explicit-alignments-robustly-improve-multilingual-encodershttpswwwaclweborganthology2020emnlp-main362-span-stylefont-size16pxwu-and-dredzespan&#34;&gt;‚≠ê 
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.362&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Do explicit alignments robustly improve multilingual encoders?&lt;/a&gt; &lt;span style=&#34;font-size:16px;&#34;&gt;(Wu and Dredze)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Unsupervised multi-lingual encoders (such as 
&lt;a href=&#34;https://github.com/google-research/bert/blob/master/multilingual.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;multilingual BERT&lt;/a&gt; and XLM-R &lt;span style=&#34;font-size:16px;&#34;&gt;
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.536&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Conneau et al., 2020b)&lt;/a&gt;&lt;/span&gt;) learn cross-lingual representations without explicit supervision. To further improve alignment between languages, Wu and Dredze propose a contrastive word-alignment objective that can utilise noisy signal (i.e., noisy parallel corpora). They show that this (marginally) outperforms other methods based on linear mapping and L2 alignment.&lt;/p&gt;
&lt;h3 id=&#34;simulated-multiple-reference-training-improves-low-resource-machine-translationhttpswwwaclweborganthology2020emnlp-main7-span-stylefont-size16pxkhayrallah-et-al-2020span&#34;&gt;
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.7/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simulated multiple reference training improves low-resource machine translation&lt;/a&gt; &lt;span style=&#34;font-size:16px;&#34;&gt;(Khayrallah et al., 2020)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;The authors propose to use target side paraphrasing to overcome data sparsity in low-resource settings. This is accomplished by incorporating a paraphraser directly into the loss function. This method works only when the target side is high-resource, since we need to train a sufficiently good paraphraser for this language. Still, it is useful for many-to-one translation, where many are low-resource languages and one is a high-resource language such as English.&lt;/p&gt;
&lt;h3 id=&#34;look-it-up-bilingual-and-monolingual-dictionaries-improve-neural-machine-translationhttpswwwaclweborganthology2020wmt-165-span-stylefont-size16pxzhong-and-chiangspan&#34;&gt;
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.wmt-1.65&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Look it up: bilingual and monolingual dictionaries improve neural machine translation&lt;/a&gt; &lt;span style=&#34;font-size:16px;&#34;&gt;(Zhong and Chiang)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Zhong and Chiang propose to attach a dictionary definition (from a dictionary dataset) to a sentence and link it to the unknown word it is defining (by sharing positional embeddings). The authors demonstrate substantial improvements using bilingual dictionaries, and minor improvements using monolingual source dictionaries.&lt;/p&gt;
&lt;h3 id=&#34;language-model-prior-for-low-resource-neural-machine-translationhttpswwwaclweborganthology2020emnlp-main615-span-stylefont-size16pxbaziotis-et-alspan&#34;&gt;
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.615&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Language model prior for low-resource neural machine translation&lt;/a&gt; &lt;span style=&#34;font-size:16px;&#34;&gt;(Baziotis et al.)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;To circumvent the scarcity of parallel data, a common solution is to exploit knowledge of a language model trained on more abundant monolingual data. Unlike earlier work that incorporates the language model in (the weights of) the decoder, this paper proposes to use a language model based regularisation term during training. Some advantages of this approach are that 1) language model constraints are soft (allowing the translation model to overrule the language model), 2) The language model is not required during decoding and 3) it is easier to deal with compared to Bayesian priors on neural network weights. While this approach seems similar to label smoothing, which could be interpreted as a uniform (or uninformative) prior, results show that label smoothing and the language model prior are complementary. The smallest monolingual corpus that is used in their experiments consists of 3M Turkish sentences. For some truly low-resource languages this amount of data is unavailable.&lt;/p&gt;
&lt;h3 id=&#34;multi-task-learning-for-multilingual-neural-machine-translationhttpswwwaclweborganthology2020emnlp-main75-span-stylefont-size16pxwang-et-alspan&#34;&gt;
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.75&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multi-task learning for multilingual neural machine translation&lt;/a&gt; &lt;span style=&#34;font-size:16px;&#34;&gt;(Wang et al.)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;The authors propose a multi-task learning framework to jointly learn from parallel and monolingual data. They optimise their translation model on three tasks: 1) standard multilingual translation task (using parallel data), 2) masked language modeling (using source-side monolingual data) and 3) denoising auto-encoding (using target-side monolingual data). Their approach outperforms a multilingual neural machine translation model (which is only trained on the translation task).&lt;/p&gt;
&lt;h3 id=&#34;when-does-unsupervised-machine-translation-workhttpswwwaclweborganthology2020wmt-168-span-stylefont-size16pxmarchisio-et-alspan&#34;&gt;
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.wmt-1.68&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;When does unsupervised machine translation work?&lt;/a&gt; &lt;span style=&#34;font-size:16px;&#34;&gt;(Marchisio et al.)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Despite the reported success of unsupervised machine translation, the authors find that unsupervised machine translation struggles with 1) dissimilar languages, 2) diverse datasets and dissimilar domains between source and target and 3) actual low-resource languages; probably because current unsupervised translation techniques &lt;span style=&#34;font-size:16px;&#34;&gt;
&lt;a href=&#34;http://aclweb.org/anthology/D18-1399&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Artetxe et al., 2018b)&lt;/a&gt;&lt;/span&gt; rely on cross-lingual embedding mapping &lt;span style=&#34;font-size:16px;&#34;&gt;
&lt;a href=&#34;http://aclweb.org/anthology/P18-1073&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Artetxe et al., 2018a)&lt;/a&gt;&lt;/span&gt; which fails under low data conditions. All the points that unsupervised machine translation struggles with are extremely relevant in real-world low-resource scenarios, which indicates that we are far away from useful unsupervised translation.&lt;/p&gt;
&lt;h3 id=&#34;pre-training-multilingual-neural-machine-translation-by-leveraging-alignment-informationhttpswwwaclweborganthology2020emnlp-main210-span-stylefont-size16pxlin-et-alspan&#34;&gt;
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.210&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pre-training multilingual neural machine translation by leveraging alignment information&lt;/a&gt; &lt;span style=&#34;font-size:16px;&#34;&gt;(Lin et al.)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;The authors propose unsupervised pre-training (a la BERT) for machine translation. A universal encoder-decoder model is trained on parallel data from many English-centric language pairs. To promote language invariance, source words are randomly replaced by their translation in a different random language using an unsupervised word alignment method. The authors then fine-tune on specific language pairs. Results indicate that high-resource languages benefit from low-resource languages, which is interesting because typically only low-resource languages benefit while high-resource languages suffer. The authors do not compare to typical multilingual baselines (which would have been interesting), but only to direct bilingual and mBART &lt;span style=&#34;font-size:16px;&#34;&gt;
&lt;a href=&#34;http://arxiv.org/abs/2001.08210&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Liu et al., 2020)&lt;/a&gt;&lt;/span&gt; and other massively pre-trained models.&lt;/p&gt;
&lt;h3 id=&#34;bleu-might-be-guilty-but-references-are-not-innocenthttpswwwaclweborganthology2020emnlp-main5-span-stylefont-size16pxfreitag-et-alspan&#34;&gt;
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BLEU might be guilty but references are not innocent&lt;/a&gt; &lt;span style=&#34;font-size:16px;&#34;&gt;(Freitag et al.)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Maximizing BLEU means aiming for the most average human translation. When translating, people tend to follow how the original sentence was, but this is not what people prefer when evaluating machine translation. Instead of improving the metric, the authors asked linguists to paraphrase existing reference translations, so they are less monotonic and do not look like the source sentences. With the new reference sentences, automatic evaluation metrics correlate much better with human judgment. Of course, human paraphrasing does not scale. Automatic paraphrasing unfortunately does not seem to offer the same benefits.&lt;/p&gt;
&lt;h3 id=&#34;multi-unit-transformers-for-neural-machine-translationhttpswwwaclweborganthology2020emnlp-main77-span-stylefont-size16pxyan-et-alspan&#34;&gt;
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.77&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multi-unit Transformers for neural machine translation&lt;/a&gt; &lt;span style=&#34;font-size:16px;&#34;&gt;(Yan et al.)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Instead of deepening the Transformer by stacking units in a cascade, the authors propose to focus on diverse and complementary parallel units which aim to promote model expressiveness. To force units to be diverse and complementary, the authors create information gaps between units by adding a bias (swapping, disorder, masking, identity). Experimental results on machine translation tasks show that the proposed model outperforms strong baselines at the cost of a small drop in inference speed (around 3 percent). The model is inherently parallelizable, but the authors do not seem to use this property.&lt;/p&gt;
&lt;h3 id=&#34;monolingual-adapters-for-zero-shot-neural-machine-translationhttpswwwaclweborganthology2020emnlp-main361-span-stylefont-size16pxphilip-et-alspan&#34;&gt;
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.361&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Monolingual adapters for zero-shot neural machine translation&lt;/a&gt; &lt;span style=&#34;font-size:16px;&#34;&gt;(Philip et al.)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Adapter layers for language direction have been shown to work in multilingual neural machine translation. The idea is to train a multilingual model with shared encoder and decoder, freeze its parameters and then insert language-pair specific adapter layers and optimise using the relevant language pair. The authors argue that this approach is parameter-inefficient (it grows quadratically with the number of languages) and offers no zero-shot benefits. They propose an adapter layer that adapts to source language in the encoder, and target language in the decoder. This approach is more parameter efficient (grows linearly) and is additionally capable of zero-shot improvements, as demonstrated by comparing to a strong multilingual baseline.&lt;/p&gt;
&lt;h3 id=&#34;subword-segmentation-and-a-single-bridge-language-affect-zero-shot-neural-machine-translationhttpswwwaclweborganthology2020wmt-164-span-stylefont-size16pxrios-et-alspan&#34;&gt;
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.wmt-1.64&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Subword segmentation and a single bridge language affect zero-shot neural machine translation&lt;/a&gt; &lt;span style=&#34;font-size:16px;&#34;&gt;(Rios et al.)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;The authors investigate common issues for zero-shot translation in multilingual neural machine translation. For zero-shot directions, the model often ignores the language tag. The authors observe a bias towards copying the source, and they show this can be mitigated by using language-specific subword segmentation. Another bias is translating into English, which happens because the model is trained on English-centric data. They show that this bias can be effectively reduced with a small amount of parallel data for non-English pairs.&lt;/p&gt;
&lt;h3 id=&#34;improving-multilingual-models-with-language-clustered-vocabularieshttpswwwaclweborganthology2020emnlp-main367-span-stylefont-size16pxchung-et-alspan&#34;&gt;
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.367&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Improving multilingual models with language-clustered vocabularies&lt;/a&gt; &lt;span style=&#34;font-size:16px;&#34;&gt;(Chung et al.)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Current multilingual models (
&lt;a href=&#34;https://github.com/google-research/bert/blob/master/multilingual.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;multilingual BERT&lt;/a&gt;, XLM-R &lt;span style=&#34;font-size:16px;&#34;&gt;
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.536&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Conneau et al., 2020b)&lt;/a&gt;&lt;/span&gt;) use a single vocabulary, generated using BPE or SentencePiece, that is shared across all languages. The authors argue that this approach is not ideal for multilingualism, because the token selection process depends on co-occurrence frequencies in the combined multilingual corpus. As a result the vocabulary will be dominated by languages that share a common script. Language freedom and cross-lingual subword sharing needs to be balanced. They propose to first cluster languages based on a vocabulary similarity metric, then learn a shared vocabulary for these clusters and finally combine all cluster vocabularies into a single multilingual vocabulary. Experiments show that all languages benefit. In particular, large gains are observed for languages that do not have a dominant script, such as Japanese.&lt;/p&gt;
&lt;h3 id=&#34;identifying-elements-essential-for-berts-multilingualityhttpswwwaclweborganthology2020emnlp-main358-span-stylefont-size16pxdufter-and-sch√ºtzespan&#34;&gt;
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.358&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Identifying elements essential for BERT‚Äôs multilinguality&lt;/a&gt; &lt;span style=&#34;font-size:16px;&#34;&gt;(Dufter and Sch√ºtze)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;The authors investigate why 
&lt;a href=&#34;https://github.com/google-research/bert/blob/master/multilingual.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;multilingual BERT&lt;/a&gt; is multilingual, even though it is not trained with any explicit multilingual signal. They find that 1) shared position embeddings, shared special tokens, replacing masked tokens with random tokens and a limited amount of parameters are necessary elements for multilinguality. (Overparameterized models have the capacity to model languages separately, so there is no incentive to align the language spaces.) 2) Having a language with the exact same structure, only with inverted order, seems to block BERT from creating a multilingual space. Having a similar word order structure is necessary for BERT to create a multilingual space. 3) The more comparable a training corpus is across languages, the higher the multilinguality. (Ideally the corpus is parallel.)&lt;/p&gt;
&lt;h3 id=&#34;from-zero-to-hero-on-the-limitations-of-zero-shot-language-transfer-with-multilingual-transformershttpswwwaclweborganthology2020emnlp-main363span-stylefont-size16pxlauscher-et-alspan&#34;&gt;
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.363&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;From zero to hero: on the limitations of zero-shot language transfer with multilingual Transformers&lt;/a&gt;&lt;span style=&#34;font-size:16px;&#34;&gt;(Lauscher et al.)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;In this paper, the authors focus on the current state-of-the-art paradigm for cross-lingual transfer in language models, where a massively multilingual transformer is pre-trained using disjoint languages, fine-tuned on a target task in English and finally evaluated on the same target task but using a (typically low-resource) language other than English. The authors find that language pre-training corpora sizes are stronger features for predicting zero-shot performance in higher-level tasks (NLI, QA), whereas results in lower-level tasks (POS tagging, dependency parsing, NER) are more affected by typological language proximity. Additionally, it is shown that additional fine-tuning on a small number of target language instances (few-shot transfer) is every effective. The authors argue that few-shot transfer is a cost-effective approach for multilingualism.&lt;/p&gt;
&lt;h3 id=&#34;lareqa-language-agnostic-answer-retrieval-from-a-multilingual-poolhttpswwwaclweborganthology2020emnlp-main477-span-stylefont-size16pxroy-et-alspan&#34;&gt;
&lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.477&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LAReQA: language-agnostic answer retrieval from a multilingual pool&lt;/a&gt; &lt;span style=&#34;font-size:16px;&#34;&gt;(Roy et al.)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;The authors present LAReQA, a benchmark for language-agnostic answer retrieval from a multilingual candidate pool. They show that high scores can be obtained for previous cross-lingual benchmarks even if the model is weakly aligned; i.e. an incorrect answer in the same language is preferred over the correct answer in a different language. LAReQA instead tests for strong alignment, requiring semantically related cross-language pairs to be closer in representation space than unrelated same-language pairs. Building on 
&lt;a href=&#34;https://github.com/google-research/bert/blob/master/multilingual.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;multilingual BERT&lt;/a&gt;, the authors find that augmenting training data via machine translation is an effective approach.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
