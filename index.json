[{"authors":["admin"],"categories":null,"content":"I am David Stap, a PhD student in the Language Technology Lab at the University of Amsterdam. I\u0026rsquo;m happy to be supervised by Christof Monz.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://davidstap.github.io/author/david-stap/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/david-stap/","section":"authors","summary":"I am David Stap, a PhD student in the Language Technology Lab at the University of Amsterdam. I\u0026rsquo;m happy to be supervised by Christof Monz.","tags":null,"title":"David Stap","type":"authors"},{"authors":null,"categories":null,"content":" EMNLP 2020, my second online NLP conference, is over. An improvement over ACL were the joint Q\u0026amp;A Zoom sessions (no more awkward empty rooms). Even better was Gather.Town (which reminded me of Habbo Hotel). It was fun to walk around from poster to poster. Let\u0026rsquo;s see what future conferences bring! Below are some notes for 20 papers that I found interesting:\nInteresting papers presented at EMNLP 2020 ‚≠ê Bridging linguistic typology and multilingual machine translation with multi-view language representations (Oncevay et al.) The authors show that Singular Vector Canonical Correlation Analysis (SVCCA) (Raghu et al., 2017) can be used to fuse linguistic typology knowledge base features with multilingual NMT-leared embeddings without diminishing the encoded original information. They show that this method can be successfully applied to language clustering, where the main idea is to obtain smaller multilingual models (for many-to-one) as an intermediate point between maintaining many pairwise systems and a single massively multilingual model. In earlier work, SVCCA is also used to study massively multilingual NMT representations (Kudugunta et al., 2019).\n‚≠ê Complete multilingual neural machine translation (Freitag and Firat) Freitag and Firat move away from English-centric multilingual data to alleviate zero-shot translation errors. They do this by mining hidden multi-way aligned examples from an English-centric dataset, and succeed in generating a complete training graph between all languages. Results indicate that translations between non-English languages benefit from the multi-way data. It is great to know that, besides data that is multi-way by design, one can obtain a lot of multi-way data hidden in parallel corpora. The same trick could be applied to other high-resource languages that act as a bridge, such as Spanish.\n‚≠ê Revisiting modularized multilingual NMT to meet industrial demands (Lyu et al.) The complete parameter sharing approach is currently dominant in multilingual machine translation, but it suffers from a capacity bottleneck and adding new languages is troublesome because retraining is required. In this paper the authors revisit the modularized setup (language specific encoders and decoders) and argue for its appropriateness in industry. They propose a model that is trained on all parallel directions (as opposed to English-centric) without sharing attention among decoders (because the semantic space is already implicitly shared by training on all language directions). That is, they use multi-way aligned data exclusively. This approach outperforms complete sharing and bilingual approaches. Additionally, in the modularized setup high-resource languages benefit from low-resource languages, which is not the case for complete sharing. Finally, the authors find that the performance of zero-shot directions outperform pivoted baselines.\n‚≠ê Participatory research for low-resourced machine translation: a case study in African languages (Nekoto et al.) The authors (47!) argue that low-resourcedness is a complex problem going beyond data availability and reflects systemic problems in society. Machine translation researchers cannot solve this problem alone, therefore participatory research is introduced as a means to involve all necessary stakeholders required in the machine translation development process. The feasibility and scalability of this approach is demonstrated with a case study on machine translation for African languages, the result of which is a collection of novel translation datasets and benchmarks for over 30 African languages. To learn more, check out their great WMT keynote.\n‚≠ê On negative interference in multilingual models: findings and a meta-learning treatment (Wang et al.) In this paper, two possible sources for negative interference in a multilingual language model (XLM-R (Conneau et al., 2020b)) are identified: 1) by sampling batches during training and computing gradient cosine similarities during training, the authors find that gradient conflict does exist for dissimilar languages, and they roughly reflect language proximity; 2) the authors observe language-specific parameters which are a potential cause for interference, but adding language-specific parameters hurts zero-shot transfer. Guided by these insights, the authors propose a meta-learning objective that effectively mitigates negative interference and improves transferability of shared layers. This objective requires calculating higher order gradients, which enable signals from all languages passing through language specific layers without sacrificing transferability in non-shared layers. Note that this work studies a bilingual setup, and it is unclear whether the approach scales to a larger number of languages.\n‚≠ê Do explicit alignments robustly improve multilingual encoders? (Wu and Dredze) Unsupervised multi-lingual encoders (such as multilingual BERT and XLM-R (Conneau et al., 2020b)) learn cross-lingual representations without explicit supervision. To further improve alignment between languages, Wu and Dredze propose a contrastive word-alignment objective that can utilise noisy signal (i.e., noisy parallel corpora). They show that this (marginally) outperforms other methods based on linear mapping and L2 alignment.\nSimulated multiple reference training improves low-resource machine translation (Khayrallah et al., 2020) The authors propose to use target side paraphrasing to overcome data sparsity in low-resource settings. This is accomplished by incorporating a paraphraser directly into the loss function. This method works only when the target side is high-resource, since we need to train a sufficiently good paraphraser for this language. Still, it is useful for many-to-one translation, where many are low-resource languages and one is a high-resource language such as English.\nLook it up: bilingual and monolingual dictionaries improve neural machine translation (Zhong and Chiang) Zhong and Chiang propose to attach a dictionary definition (from a dictionary dataset) to a sentence and link it to the unknown word it is defining (by sharing positional embeddings). The authors demonstrate substantial improvements using bilingual dictionaries, and minor improvements using monolingual source dictionaries.\nLanguage model prior for low-resource neural machine translation (Baziotis et al.) To circumvent the scarcity of parallel data, a common solution is to exploit knowledge of a language model trained on more abundant monolingual data. Unlike earlier work that incorporates the language model in (the weights of) the decoder, this paper proposes to use a language model based regularisation term during training. Some advantages of this approach are that 1) language model constraints are soft (allowing the translation model to overrule the language model), 2) The language model is not required during decoding and 3) it is easier to deal with compared to Bayesian priors on neural network weights. While this approach seems similar to label smoothing, which could be interpreted as a uniform (or uninformative) prior, results show that label smoothing and the language model prior are complementary. The smallest monolingual corpus that is used in their experiments consists of 3M Turkish sentences. For some truly low-resource languages this amount of data is unavailable.\nMulti-task learning for multilingual neural machine translation (Wang et al.) The authors propose a multi-task learning framework to jointly learn from parallel and monolingual data. They optimise their translation model on three tasks: 1) standard multilingual translation task (using parallel data), 2) masked language modeling (using source-side monolingual data) and 3) denoising auto-encoding (using target-side monolingual data). Their approach outperforms a multilingual neural machine translation model (which is only trained on the translation task).\nWhen does unsupervised machine translation work? (Marchisio et al.) Despite the reported success of unsupervised machine translation, the authors find that unsupervised machine translation struggles with 1) dissimilar languages, 2) diverse datasets and dissimilar domains between source and target and 3) actual low-resource languages; probably because current unsupervised translation techniques (Artetxe et al., 2018b) rely on cross-lingual embedding mapping (Artetxe et al., 2018a) which fails under low data conditions. All the points that unsupervised machine translation struggles with are extremely relevant in real-world low-resource scenarios, which indicates that we are far away from useful unsupervised translation.\nPre-training multilingual neural machine translation by leveraging alignment information (Lin et al.) The authors propose unsupervised pre-training (a la BERT) for machine translation. A universal encoder-decoder model is trained on parallel data from many English-centric language pairs. To promote language invariance, source words are randomly replaced by their translation in a different random language using an unsupervised word alignment method. The authors then fine-tune on specific language pairs. Results indicate that high-resource languages benefit from low-resource languages, which is interesting because typically only low-resource languages benefit while high-resource languages suffer. The authors do not compare to typical multilingual baselines (which would have been interesting), but only to direct bilingual and mBART (Liu et al., 2020) and other massively pre-trained models.\nBLEU might be guilty but references are not innocent (Freitag et al.) Maximizing BLEU means aiming for the most average human translation. When translating, people tend to follow how the original sentence was, but this is not what people prefer when evaluating machine translation. Instead of improving the metric, the authors asked linguists to paraphrase existing reference translations, so they are less monotonic and do not look like the source sentences. With the new reference sentences, automatic evaluation metrics correlate much better with human judgment. Of course, human paraphrasing does not scale. Automatic paraphrasing unfortunately does not seem to offer the same benefits.\nMulti-unit Transformers for neural machine translation (Yan et al.) Instead of deepening the Transformer by stacking units in a cascade, the authors propose to focus on diverse and complementary parallel units which aim to promote model expressiveness. To force units to be diverse and complementary, the authors create information gaps between units by adding a bias (swapping, disorder, masking, identity). Experimental results on machine translation tasks show that the proposed model outperforms strong baselines at the cost of a small drop in inference speed (around 3 percent). The model is inherently parallelizable, but the authors do not seem to use this property.\nMonolingual adapters for zero-shot neural machine translation (Philip et al.) Adapter layers for language direction have been shown to work in multilingual neural machine translation. The idea is to train a multilingual model with shared encoder and decoder, freeze its parameters and then insert language-pair specific adapter layers and optimise using the relevant language pair. The authors argue that this approach is parameter-inefficient (it grows quadratically with the number of languages) and offers no zero-shot benefits. They propose an adapter layer that adapts to source language in the encoder, and target language in the decoder. This approach is more parameter efficient (grows linearly) and is additionally capable of zero-shot improvements, as demonstrated by comparing to a strong multilingual baseline.\nSubword segmentation and a single bridge language affect zero-shot neural machine translation (Rios et al.) The authors investigate common issues for zero-shot translation in multilingual neural machine translation. For zero-shot directions, the model often ignores the language tag. The authors observe a bias towards copying the source, and they show this can be mitigated by using language-specific subword segmentation. Another bias is translating into English, which happens because the model is trained on English-centric data. They show that this bias can be effectively reduced with a small amount of parallel data for non-English pairs.\nImproving multilingual models with language-clustered vocabularies (Chung et al.) Current multilingual models ( multilingual BERT, XLM-R (Conneau et al., 2020b)) use a single vocabulary, generated using BPE or SentencePiece, that is shared across all languages. The authors argue that this approach is not ideal for multilingualism, because the token selection process depends on co-occurrence frequencies in the combined multilingual corpus. As a result the vocabulary will be dominated by languages that share a common script. Language freedom and cross-lingual subword sharing needs to be balanced. They propose to first cluster languages based on a vocabulary similarity metric, then learn a shared vocabulary for these clusters and finally combine all cluster vocabularies into a single multilingual vocabulary. Experiments show that all languages benefit. In particular, large gains are observed for languages that do not have a dominant script, such as Japanese.\nIdentifying elements essential for BERT‚Äôs multilinguality (Dufter and Sch√ºtze) The authors investigate why multilingual BERT is multilingual, even though it is not trained with any explicit multilingual signal. They find that 1) shared position embeddings, shared special tokens, replacing masked tokens with random tokens and a limited amount of parameters are necessary elements for multilinguality. (Overparameterized models have the capacity to model languages separately, so there is no incentive to align the language spaces.) 2) Having a language with the exact same structure, only with inverted order, seems to block BERT from creating a multilingual space. Having a similar word order structure is necessary for BERT to create a multilingual space. 3) The more comparable a training corpus is across languages, the higher the multilinguality. (Ideally the corpus is parallel.)\nFrom zero to hero: on the limitations of zero-shot language transfer with multilingual Transformers(Lauscher et al.) In this paper, the authors focus on the current state-of-the-art paradigm for cross-lingual transfer in language models, where a massively multilingual transformer is pre-trained using disjoint languages, fine-tuned on a target task in English and finally evaluated on the same target task but using a (typically low-resource) language other than English. The authors find that language pre-training corpora sizes are stronger features for predicting zero-shot performance in higher-level tasks (NLI, QA), whereas results in lower-level tasks (POS tagging, dependency parsing, NER) are more affected by typological language proximity. Additionally, it is shown that additional fine-tuning on a small number of target language instances (few-shot transfer) is every effective. The authors argue that few-shot transfer is a cost-effective approach for multilingualism.\nLAReQA: language-agnostic answer retrieval from a multilingual pool (Roy et al.) The authors present LAReQA, a benchmark for language-agnostic answer retrieval from a multilingual candidate pool. They show that high scores can be obtained for previous cross-lingual benchmarks even if the model is weakly aligned; i.e. an incorrect answer in the same language is preferred over the correct answer in a different language. LAReQA instead tests for strong alignment, requiring semantically related cross-language pairs to be closer in representation space than unrelated same-language pairs. Building on multilingual BERT, the authors find that augmenting training data via machine translation is an effective approach.\n","date":1606086000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606086000,"objectID":"50f71408acd2d533dace0ca721f36151","permalink":"https://davidstap.github.io/post/emnlp/","publishdate":"2020-11-23T00:00:00+01:00","relpermalink":"/post/emnlp/","section":"post","summary":"EMNLP 2020, my second online NLP conference, is over. An improvement over ACL were the joint Q\u0026amp;A Zoom sessions (no more awkward empty rooms). Even better was Gather.Town (which reminded me of Habbo Hotel).","tags":null,"title":"Some EMNLP 2020 notes üèùÔ∏èüá©üá¥üíª","type":"post"},{"authors":["David Stap, Maurits Bleeker, Sarah Ibrahimi, Maartje ter Hoeve"],"categories":[],"content":"","date":1588118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588118400,"objectID":"eada40c10406e638d1e541a2e544ec84","permalink":"https://davidstap.github.io/publication/2020a-cvpr/","publishdate":"2020-04-29T00:00:00Z","relpermalink":"/publication/2020a-cvpr/","section":"publication","summary":"In recent years, Generative Adversarial Networks (GANs) have improved steadily towards generating increasingly impressive real-world images. It is useful to steer the image generation process for purposes such as content creation. This can be done by conditioning the model on additional information. However, when conditioning on additional information, there still exists a large set of images that agree with a particular conditioning. This makes it unlikely that the generated image is exactly as envisioned by a user, which is problematic for practical content creation scenarios such as generating facial composites or stock photos.  To solve this problem, we propose a single pipeline for text-to-image generation and manipulation. In the first part of our pipeline we introduce textStyleGAN, a model that is conditioned on text. In the second part of our pipeline we make use of the pre-trained weights of textStyleGAN to perform semantic facial image manipulation. The approach works by finding semantic directions in latent space. We show that this method can be used to manipulate facial images for a wide range of attributes. Finally, we introduce the CelebTD-HQ dataset, an extension to CelebA-HQ, consisting of faces and corresponding textual descriptions. ","tags":[],"title":"Conditional image generation and manipulation for user-specified content","type":"publication"},{"authors":["David Stap, Bert Bredeweg, Natasa Brouwer"],"categories":[],"content":"","date":1586476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586476800,"objectID":"47083aaf308a1323dc5202a46bfad253","permalink":"https://davidstap.github.io/publication/2017b-bnaic/","publishdate":"2020-04-10T00:00:00Z","relpermalink":"/publication/2017b-bnaic/","section":"publication","summary":"Learning analytics aims to optimise learning, typically by providing students meaningful insight in their own learning behaviour. Gamification deploys game mechanics to increase motivation and thereby boost the learning process. In our work, we use learning analytics to implement game mechanics that create a motivating learning experience. The educational context concerns students that engage in model-building to develop systems thinking expertise. Three mechanics have been implemented: badges, leaderboard and life. The gamification add-on was evaluated during high school physics classes. Data mining showed that gamification resulted in significantly higher self-reported scores on enjoyment but inferior student-created models. A strong correlation between delete-behaviour and correctness of the created models was also found.","tags":[],"title":"Gamification for learning by modelling in interactive learning environments","type":"publication"}]